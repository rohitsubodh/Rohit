{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHTpqZaRsmPT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pq9aM4kwyv_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37629030-a216-4d6f-ba63-7a317ce90fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P7bTO7bvtMrh"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SPQOTFpLtM0_"
      },
      "outputs": [],
      "source": [
        "!pip install -q keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cypQyfh2pWP",
        "outputId": "b9e6f3dc-5641-4837-9121-0f2ff8bda0e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-f7_nqfvd\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-f7_nqfvd\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.7.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101077 sha256=194f10ad6fe9805fa76e3a957f7fc017837f9e8a46717fe854e63b70c3afe813\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sb4yesx3/wheels/bb/1f/f2/b57495012683b6b20bbae94a3915ec79753111452d79886abc\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WwRSYNy1tM3x"
      },
      "outputs": [],
      "source": [
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "al3TxZontIc_"
      },
      "outputs": [],
      "source": [
        "from random import random\n",
        "from numpy import load\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import asarray\n",
        "from numpy.random import randint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.models import Model\n",
        "from keras.models import Input\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Concatenate\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5TLWlRynWCER"
      },
      "outputs": [],
      "source": [
        "\n",
        "# define the discriminator model\n",
        "def define_discriminator(image_shape):\n",
        "# weight initialization\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "# source image input\n",
        "  in_image = Input(shape=image_shape)\n",
        "# C64\n",
        "  d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "# C128\n",
        "  d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "  d = InstanceNormalization(axis=-1)(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "# C256\n",
        "  d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "  d = InstanceNormalization(axis=-1)(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "# C512\n",
        "  d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "  d = InstanceNormalization(axis=-1)(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "# second last output layer\n",
        "  d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "  d = InstanceNormalization(axis=-1)(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "# patch output\n",
        "  patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "# define model\n",
        "  model = Model(in_image, patch_out)\n",
        "# compile model\n",
        "  model.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5])\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FNb32zhkWfRz"
      },
      "outputs": [],
      "source": [
        "# generator a resnet block\n",
        "def resnet_block(n_filters, input_layer):\n",
        "# weight initialization\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "# first layer convolutional layer\n",
        "  g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n",
        "  g = InstanceNormalization(axis=-1)(g)\n",
        "  g = Activation('relu')(g)\n",
        "# second convolutional layer\n",
        "  g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n",
        "  g = InstanceNormalization(axis=-1)(g)\n",
        "# concatenate merge channel-wise with input layer\n",
        "  g = Concatenate()([g, input_layer])\n",
        "  return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ek8KQa1iWCSn"
      },
      "outputs": [],
      "source": [
        "# define the standalone generator model\n",
        "def define_generator(image_shape, n_resnet=9):\n",
        "  # weight initialization\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "# image input\n",
        "  in_image = Input(shape=image_shape)\n",
        "# c7s1-64\n",
        "  g = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n",
        "  g = InstanceNormalization(axis=-1)(g)\n",
        "  g = Activation('relu')(g)\n",
        "# d128\n",
        "  g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
        "  g = InstanceNormalization(axis=-1)(g)\n",
        "  g = Activation('relu')(g)\n",
        "# d256\n",
        "  g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
        "  g = InstanceNormalization(axis=-1)(g)\n",
        "  g = Activation('relu')(g)\n",
        "# R256\n",
        "  for _ in range(n_resnet):\n",
        "     g = resnet_block(256, g)\n",
        "# u128\n",
        "  g = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
        "  g = InstanceNormalization(axis=-1)(g)\n",
        "  g = Activation('relu')(g)\n",
        "# u64\n",
        "  g = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
        "  g = InstanceNormalization(axis=-1)(g)\n",
        "  g = Activation('relu')(g)\n",
        "# c7s1-3\n",
        "  g = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n",
        "  g = InstanceNormalization(axis=-1)(g)\n",
        "  out_image = Activation('tanh')(g)\n",
        "# define model\n",
        "  model = Model(in_image, out_image)\n",
        "  return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a composite model for updating generators by adversarial and cycle loss\n",
        "def define_composite_model(g_model_1, d_model, g_model_2, image_shape):\n",
        "# ensure the model weâœ¬re updating is trainable\n",
        "    g_model_1.trainable = True\n",
        "# mark discriminator as not trainable\n",
        "    d_model.trainable = False\n",
        "# mark other generator model as not trainable\n",
        "    g_model_2.trainable = False\n",
        "# discriminator element\n",
        "    input_gen = Input(shape=image_shape)\n",
        "    gen1_out = g_model_1(input_gen)\n",
        "    output_d = d_model(gen1_out)\n",
        "# identity element\n",
        "    input_id = Input(shape=image_shape)\n",
        "    output_id = g_model_1(input_id)\n",
        "# forward cycle\n",
        "    output_f = g_model_2(gen1_out)\n",
        "# backward cycle\n",
        "    gen2_out = g_model_2(input_id)\n",
        "    output_b = g_model_1(gen2_out)\n",
        "# define model graph\n",
        "    model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
        "# define optimization algorithm configuration\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "# compile model with weighting of least squares loss and L1 loss\n",
        "    model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10],\n",
        "        optimizer=opt)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "2ZGfBUhszPp4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Qub-1MebWCUW"
      },
      "outputs": [],
      "source": [
        "# load and prepare training images\n",
        "def load_real_samples(filename):\n",
        "# load the dataset\n",
        "  data = load(filename)\n",
        "# unpack arrays\n",
        "  X1, X2 = data['arr_0'], data['arr_1']\n",
        "# scale from [0,255] to [-1,1]\n",
        "  X1 = (X1 - 127.5) / 127.5\n",
        "  X2 = (X2 - 127.5) / 127.5\n",
        "  return [X1, X2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LqJiKW7YWCW9"
      },
      "outputs": [],
      "source": [
        "# select a batch of random samples, returns images and target\n",
        "def generate_real_samples(dataset, n_samples, patch_shape):\n",
        "# choose random instances\n",
        "  ix = randint(0, dataset.shape[0], n_samples)\n",
        "# retrieve selected images\n",
        "  X = dataset[ix]\n",
        "# generate âœ¬realâœ¬ class labels (1)\n",
        "  y = ones((n_samples, patch_shape, patch_shape, 1))\n",
        "  return X, y\n",
        "\n",
        "# generate a batch of images, returns images and targets\n",
        "def generate_fake_samples(g_model, dataset, patch_shape):\n",
        "# generate fake instance\n",
        "  X = g_model.predict(dataset)\n",
        "# create âœ¬fakeâœ¬ class labels (0)\n",
        "  y = zeros((len(X), patch_shape, patch_shape, 1))\n",
        "  return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-a-MwCAcWCZx"
      },
      "outputs": [],
      "source": [
        "# save the generator models to file\n",
        "def save_models(step, g_model_AtoB, g_model_BtoA):\n",
        "# save the first generator model\n",
        "  filename1 = 'g_model_AtoB_%06d.h5' % (step+1)\n",
        "  g_model_AtoB.save(filename1)\n",
        "# save the second generator model\n",
        "  filename2 = 'g_model_BtoA_%06d.h5' % (step+1)\n",
        "  g_model_BtoA.save(filename2)\n",
        "  print('>Saved: %s and %s' % (filename1, filename2))\n",
        "\n",
        "# generate samples and save as a plot and save the model\n",
        "def summarize_performance(step, g_model, trainX, name, n_samples=5):\n",
        "# select a sample of input images\n",
        "  X_in, _ = generate_real_samples(trainX, n_samples, 0)\n",
        "# generate translated images\n",
        "  X_out, _ = generate_fake_samples(g_model, X_in, 0)\n",
        "# scale all pixels from [-1,1] to [0,1]\n",
        "  X_in = (X_in + 1) / 2.0\n",
        "  X_out = (X_out + 1) / 2.0\n",
        "# plot real images\n",
        "  for i in range(n_samples):\n",
        "     pyplot.subplot(2, n_samples, 1 + i)\n",
        "     pyplot.axis('off')\n",
        "     pyplot.imshow(X_in[i])\n",
        "# plot translated image\n",
        "  for i in range(n_samples):\n",
        "     pyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
        "     pyplot.axis('off')\n",
        "     pyplot.imshow(X_out[i])\n",
        "# save plot to file\n",
        "  filename1 = '%s_generated_plot_%06d.png' % (name, (step+1))\n",
        "  pyplot.savefig(filename1)\n",
        "  pyplot.close()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s5btNXRpWCdU"
      },
      "outputs": [],
      "source": [
        "# update image pool for fake images\n",
        "def update_image_pool(pool, images, max_size=50):\n",
        "    selected = list()\n",
        "    for image in images:\n",
        "      if len(pool) < max_size:\n",
        "# stock the pool\n",
        "        pool.append(image)\n",
        "        selected.append(image)\n",
        "      elif random() < 0.5:\n",
        "# use image, but donâœ¬t add it to the pool\n",
        "        selected.append(image)\n",
        "      else:\n",
        "# replace an existing image and use replaced image\n",
        "          ix = randint(0, len(pool))\n",
        "          selected.append(pool[ix])\n",
        "          pool[ix] = image\n",
        "    return asarray(selected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QJPcpRDebOtk"
      },
      "outputs": [],
      "source": [
        "# train cyclegan model\n",
        "def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA,\n",
        "dataset):\n",
        "# define properties of the training run\n",
        "  n_epochs, n_batch, = 10, 1\n",
        "# determine the output square shape of the discriminator\n",
        "  n_patch = d_model_A.output_shape[1]\n",
        "# unpack dataset\n",
        "  trainA, trainB = dataset\n",
        "# prepare image pool for fakes\n",
        "  poolA, poolB = list(), list()\n",
        "# calculate the number of batches per training epoch\n",
        "  bat_per_epo = int(len(trainA) / n_batch)\n",
        "# calculate the number of training iterations\n",
        "  n_steps = bat_per_epo * n_epochs\n",
        "# manually enumerate epochs\n",
        "  for i in range(n_steps):\n",
        "    # select a batch of real samples\n",
        "    X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n",
        "    X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n",
        "   # generate a batch of fake samples\n",
        "    X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n",
        "    X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n",
        "# update fakes from pool\n",
        "    X_fakeA = update_image_pool(poolA, X_fakeA)\n",
        "    X_fakeB = update_image_pool(poolB, X_fakeB)\n",
        "# update generator B->A via adversarial and cycle loss\n",
        "    g_loss2, _, _, _, _ = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA,\n",
        "          X_realA, X_realB, X_realA])\n",
        "# update discriminator for A -> [real/fake]\n",
        "    dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n",
        "    dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n",
        "# update generator A->B via adversarial and cycle loss\n",
        "    g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB,\n",
        "     X_realB, X_realA, X_realB])\n",
        "# update discriminator for B -> [real/fake]\n",
        "    dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n",
        "    dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n",
        "# summarize performance\n",
        "    print('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2,\n",
        "       dB_loss1,dB_loss2, g_loss1,g_loss2))\n",
        "    # evaluate the model performance every so often\n",
        "    if (i+1) % (bat_per_epo * 1) == 0:\n",
        "# plot A->B translation\n",
        "       summarize_performance(i, g_model_AtoB, trainA, 'AtoB')\n",
        "# plot B->A translation\n",
        "       summarize_performance(i, g_model_BtoA, trainB, 'BtoA')\n",
        "    if (i+1) % (bat_per_epo * 5) == 0:\n",
        "# save the models\n",
        "       save_models(i, g_model_AtoB, g_model_BtoA)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from numpy import asarray\n",
        "from numpy import vstack\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from numpy import savez_compressed\n",
        "# load all images in a directory into memory\n",
        "def load_images(path, size=(256,256)):\n",
        "  data_list = list()\n",
        "# enumerate filenames in directory, assume all are images\n",
        "  c=0;\n",
        "  for filename in listdir(path):\n",
        "# load and resize the image\n",
        "     if c==25:\n",
        "       break;\n",
        "     pixels = load_img(path + filename, target_size=size)\n",
        "# convert to numpy array\n",
        "     pixels = img_to_array(pixels)\n",
        "# store\n",
        "     data_list.append(pixels)\n",
        "     c=c+1;\n",
        "  return asarray(data_list)\n",
        "# dataset path\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/underwater_imagenet/'\n",
        "# pathA = '/content/gdrive/MyDrive/Colab Notebooks/underwater_imagenet/'\n",
        "# load dataset B\n",
        "\n",
        "dataB1 = load_images(path+'trainB/')\n",
        "print('Loaded dataB: ', dataB1.shape)\n",
        "# load dataset A\n",
        "# dataA1 = load_images(path+'trainA')\n",
        "# print('Loaded dataA: ', dataA1.shape)\n",
        "# # # load dataset B\n",
        "# # dataB = vstack((dataB1, dataB2))\n",
        "# # print(âœ¬Loaded dataB: âœ¬, dataB.shape)\n",
        "# # # save as compressed numpy array\n",
        "# filename = 'underwater.npz'\n",
        "# savez_compressed(filename, dataA, dataB)\n",
        "# print('Saved dataset: ', filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAfo2n_bxtFb",
        "outputId": "6ee9a5c2-70c6-4c79-f513-674c5722276d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataB:  (25, 256, 256, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0EcYR5XZwTCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d190078-98ab-45b5-d504-ccf73e8631b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataA:  (25, 256, 256, 3)\n"
          ]
        }
      ],
      "source": [
        "dataA1 = load_images(path+'trainA/')\n",
        "print('Loaded dataA: ', dataA1.shape)\n",
        "# # load dataset B\n",
        "# dataB = vstack((dataB1, dataB2))\n",
        "# print(âœ¬Loaded dataB: âœ¬, dataB.shape)\n",
        "# # save as compressed numpy array\n",
        "filename = 'underwater.npz'\n",
        "savez_compressed(filename, dataA1, dataB1)\n",
        "# print('Saved dataset: ', filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLh2S0a5cK0d",
        "outputId": "8eb25453-fce8-4068-deb3-2ce5f93f29d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded (25, 256, 256, 3) (25, 256, 256, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">1, dA[0.937,0.826] dB[0.898,1.359] g[17.223,17.838]\n",
            ">2, dA[1.765,0.826] dB[5.509,0.976] g[17.356,16.976]\n",
            ">3, dA[2.631,1.211] dB[3.031,1.132] g[17.023,17.895]\n",
            ">4, dA[2.615,1.893] dB[4.185,1.059] g[15.759,17.394]\n",
            ">5, dA[2.871,8.752] dB[2.843,0.868] g[19.428,21.198]\n",
            ">6, dA[1.905,4.464] dB[1.135,1.397] g[16.181,16.855]\n",
            ">7, dA[2.189,2.290] dB[1.264,2.237] g[15.588,15.406]\n",
            ">8, dA[1.371,2.138] dB[1.900,1.427] g[13.706,14.467]\n",
            ">9, dA[1.578,1.108] dB[1.978,1.633] g[16.127,13.659]\n",
            ">10, dA[1.314,3.408] dB[2.843,1.313] g[15.150,15.873]\n",
            ">11, dA[0.903,2.248] dB[2.188,2.174] g[12.110,14.519]\n",
            ">12, dA[0.709,2.537] dB[0.841,1.373] g[11.354,13.449]\n",
            ">13, dA[0.533,1.074] dB[0.586,1.097] g[13.135,12.066]\n",
            ">14, dA[0.685,0.480] dB[0.573,0.988] g[10.695,10.317]\n",
            ">15, dA[0.473,0.408] dB[0.585,0.517] g[11.643,10.608]\n",
            ">16, dA[0.419,0.360] dB[0.577,0.627] g[10.178,9.342]\n",
            ">17, dA[0.453,0.360] dB[0.666,0.520] g[11.655,10.973]\n",
            ">18, dA[0.433,0.284] dB[0.432,0.398] g[9.470,10.069]\n",
            ">19, dA[0.613,0.331] dB[0.471,0.285] g[8.753,9.178]\n",
            ">20, dA[0.379,0.510] dB[0.301,0.279] g[12.176,13.936]\n",
            ">21, dA[0.384,0.253] dB[0.532,0.446] g[8.937,9.024]\n",
            ">22, dA[0.390,0.366] dB[0.348,0.319] g[9.622,10.249]\n",
            ">23, dA[0.481,0.349] dB[0.431,0.306] g[8.458,9.065]\n",
            ">24, dA[0.324,0.210] dB[0.451,0.296] g[9.704,9.019]\n",
            ">25, dA[0.349,0.226] dB[0.585,0.509] g[8.502,8.539]\n",
            ">26, dA[0.437,0.303] dB[0.348,0.541] g[11.581,10.851]\n",
            ">27, dA[0.218,0.162] dB[0.304,0.314] g[8.797,9.070]\n",
            ">28, dA[0.303,0.245] dB[0.428,0.410] g[8.664,9.393]\n",
            ">29, dA[0.173,0.210] dB[0.292,0.373] g[11.042,10.186]\n",
            ">30, dA[0.208,0.255] dB[0.303,0.677] g[7.893,7.894]\n",
            ">31, dA[0.361,0.188] dB[0.362,0.438] g[8.815,9.750]\n",
            ">32, dA[0.304,0.160] dB[0.233,0.226] g[7.992,7.907]\n",
            ">33, dA[0.300,0.231] dB[0.241,0.239] g[9.430,9.357]\n",
            ">34, dA[0.128,0.166] dB[0.186,0.262] g[7.790,7.830]\n",
            ">35, dA[0.324,0.178] dB[0.128,0.320] g[7.471,7.805]\n",
            ">36, dA[0.225,0.219] dB[0.225,0.195] g[7.619,8.542]\n",
            ">37, dA[0.181,0.186] dB[0.207,0.183] g[7.691,8.520]\n",
            ">38, dA[0.204,0.155] dB[0.153,0.148] g[7.674,8.250]\n",
            ">39, dA[0.190,0.271] dB[0.260,0.232] g[9.616,8.797]\n",
            ">40, dA[0.199,0.159] dB[0.278,0.298] g[7.221,7.960]\n",
            ">41, dA[0.191,0.188] dB[0.177,0.194] g[7.079,7.095]\n",
            ">42, dA[0.203,0.220] dB[0.315,0.212] g[11.925,10.624]\n",
            ">43, dA[0.209,0.226] dB[0.201,0.186] g[7.381,8.520]\n",
            ">44, dA[0.160,0.168] dB[0.212,0.162] g[6.746,6.911]\n",
            ">45, dA[0.190,0.200] dB[0.112,0.156] g[6.363,6.762]\n",
            ">46, dA[0.104,0.188] dB[0.232,0.155] g[7.271,7.940]\n",
            ">47, dA[0.164,0.122] dB[0.168,0.153] g[7.891,8.316]\n",
            ">48, dA[0.141,0.090] dB[0.050,0.104] g[5.882,6.293]\n",
            ">49, dA[0.117,0.188] dB[0.169,0.097] g[7.557,8.068]\n",
            ">50, dA[0.120,0.183] dB[0.107,0.192] g[7.296,8.102]\n",
            ">51, dA[0.137,0.215] dB[0.362,0.143] g[7.981,7.295]\n",
            ">52, dA[0.147,0.100] dB[0.176,0.311] g[6.150,6.592]\n",
            ">53, dA[0.194,0.116] dB[0.370,0.194] g[12.881,13.648]\n",
            ">54, dA[0.082,0.160] dB[0.140,0.282] g[8.610,8.685]\n",
            ">55, dA[0.145,0.125] dB[0.094,0.125] g[6.109,7.226]\n",
            ">56, dA[0.167,0.179] dB[0.207,0.190] g[8.471,8.945]\n",
            ">57, dA[0.138,0.209] dB[0.186,0.206] g[10.342,9.979]\n",
            ">58, dA[0.177,0.113] dB[0.124,0.124] g[6.944,8.194]\n",
            ">59, dA[0.164,0.095] dB[0.140,0.111] g[10.888,12.783]\n",
            ">60, dA[0.092,0.209] dB[0.141,0.349] g[8.925,8.188]\n",
            ">61, dA[0.191,0.205] dB[0.119,0.176] g[10.569,10.341]\n",
            ">62, dA[0.097,0.124] dB[0.307,0.216] g[11.509,10.102]\n",
            ">63, dA[0.155,0.121] dB[0.150,0.159] g[8.099,8.480]\n",
            ">64, dA[0.135,0.143] dB[0.177,0.187] g[9.035,9.386]\n",
            ">65, dA[0.115,0.131] dB[0.130,0.185] g[8.624,9.217]\n",
            ">66, dA[0.118,0.085] dB[0.143,0.198] g[7.732,8.629]\n",
            ">67, dA[0.068,0.153] dB[0.147,0.121] g[8.680,8.246]\n",
            ">68, dA[0.224,0.144] dB[0.248,0.105] g[7.528,7.989]\n",
            ">69, dA[0.119,0.189] dB[0.075,0.162] g[8.608,9.290]\n",
            ">70, dA[0.085,0.090] dB[0.145,0.129] g[7.320,8.126]\n",
            ">71, dA[0.118,0.151] dB[0.069,0.092] g[6.855,7.325]\n",
            ">72, dA[0.109,0.166] dB[0.127,0.093] g[6.997,7.438]\n",
            ">73, dA[0.059,0.107] dB[0.103,0.167] g[8.271,8.681]\n",
            ">74, dA[0.136,0.073] dB[0.086,0.087] g[7.352,7.077]\n",
            ">75, dA[0.273,0.192] dB[0.351,0.160] g[7.496,7.317]\n",
            ">76, dA[0.157,0.130] dB[0.092,0.244] g[7.824,7.748]\n",
            ">77, dA[0.175,0.292] dB[0.231,0.096] g[11.549,12.987]\n",
            ">78, dA[0.182,0.101] dB[0.060,0.105] g[6.918,6.840]\n",
            ">79, dA[0.106,0.142] dB[0.121,0.231] g[8.156,8.511]\n",
            ">80, dA[0.066,0.165] dB[0.130,0.206] g[7.161,8.155]\n",
            ">81, dA[0.098,0.055] dB[0.218,0.158] g[5.400,5.907]\n",
            ">82, dA[0.200,0.351] dB[0.153,0.150] g[8.631,7.641]\n",
            ">83, dA[0.082,0.106] dB[0.189,0.133] g[10.948,9.529]\n",
            ">84, dA[0.169,0.089] dB[0.100,0.112] g[5.757,6.200]\n",
            ">85, dA[0.107,0.100] dB[0.164,0.228] g[5.815,6.726]\n",
            ">86, dA[0.053,0.134] dB[0.140,0.174] g[7.259,7.452]\n",
            ">87, dA[0.064,0.056] dB[0.092,0.065] g[8.916,8.525]\n",
            ">88, dA[0.106,0.082] dB[0.140,0.198] g[6.328,7.708]\n",
            ">89, dA[0.128,0.072] dB[0.141,0.103] g[5.362,5.735]\n",
            ">90, dA[0.062,0.061] dB[0.186,0.090] g[6.749,6.995]\n",
            ">91, dA[0.120,0.141] dB[0.124,0.182] g[14.160,12.236]\n",
            ">92, dA[0.091,0.165] dB[0.078,0.123] g[8.616,8.347]\n",
            ">93, dA[0.126,0.132] dB[0.173,0.112] g[6.547,7.132]\n",
            ">94, dA[0.223,0.104] dB[0.090,0.099] g[5.869,6.326]\n",
            ">95, dA[0.083,0.243] dB[0.121,0.172] g[7.267,8.036]\n",
            ">96, dA[0.061,0.049] dB[0.200,0.099] g[7.629,7.560]\n",
            ">97, dA[0.099,0.184] dB[0.238,0.244] g[8.195,8.273]\n",
            ">98, dA[0.179,0.116] dB[0.110,0.157] g[6.226,6.609]\n",
            ">99, dA[0.217,0.178] dB[0.113,0.068] g[7.089,7.959]\n",
            ">100, dA[0.121,0.173] dB[0.254,0.210] g[6.048,6.079]\n",
            ">101, dA[0.077,0.162] dB[0.218,0.180] g[8.502,8.901]\n",
            ">102, dA[0.175,0.084] dB[0.144,0.107] g[6.731,7.285]\n",
            ">103, dA[0.119,0.069] dB[0.213,0.110] g[6.862,7.284]\n",
            ">104, dA[0.166,0.054] dB[0.091,0.144] g[6.665,7.293]\n",
            ">105, dA[0.104,0.123] dB[0.102,0.052] g[6.803,7.475]\n",
            ">106, dA[0.170,0.074] dB[0.171,0.121] g[10.976,10.055]\n",
            ">107, dA[0.227,0.257] dB[0.109,0.110] g[10.672,9.259]\n",
            ">108, dA[0.109,0.230] dB[0.153,0.234] g[6.937,6.675]\n",
            ">109, dA[0.166,0.159] dB[0.147,0.195] g[6.510,7.777]\n",
            ">110, dA[0.135,0.035] dB[0.149,0.094] g[7.339,7.414]\n",
            ">111, dA[0.031,0.069] dB[0.114,0.087] g[8.346,7.955]\n",
            ">112, dA[0.062,0.080] dB[0.086,0.164] g[7.047,7.534]\n",
            ">113, dA[0.233,0.169] dB[0.126,0.094] g[7.879,8.088]\n",
            ">114, dA[0.089,0.294] dB[0.193,0.125] g[9.855,9.078]\n",
            ">115, dA[0.073,0.053] dB[0.063,0.165] g[6.861,7.857]\n",
            ">116, dA[0.127,0.053] dB[0.120,0.115] g[7.079,7.915]\n",
            ">117, dA[0.099,0.096] dB[0.077,0.132] g[7.597,8.529]\n",
            ">118, dA[0.054,0.051] dB[0.294,0.078] g[7.475,7.386]\n",
            ">119, dA[0.111,0.073] dB[0.067,0.270] g[5.826,7.567]\n",
            ">120, dA[0.086,0.100] dB[0.074,0.028] g[9.907,11.668]\n",
            ">121, dA[0.059,0.149] dB[0.101,0.068] g[6.623,7.791]\n",
            ">122, dA[0.186,0.149] dB[0.134,0.151] g[11.307,9.936]\n",
            ">123, dA[0.104,0.100] dB[0.069,0.220] g[7.738,8.095]\n",
            ">124, dA[0.171,0.170] dB[0.151,0.120] g[11.088,9.513]\n",
            ">125, dA[0.086,0.124] dB[0.095,0.140] g[7.848,7.933]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            ">Saved: g_model_AtoB_000125.h5 and g_model_BtoA_000125.h5\n",
            ">126, dA[0.152,0.132] dB[0.073,0.215] g[5.680,6.307]\n",
            ">127, dA[0.068,0.116] dB[0.077,0.088] g[8.095,8.553]\n",
            ">128, dA[0.049,0.072] dB[0.114,0.137] g[8.036,8.510]\n",
            ">129, dA[0.230,0.079] dB[0.158,0.105] g[11.314,9.723]\n",
            ">130, dA[0.085,0.142] dB[0.220,0.180] g[8.083,8.700]\n",
            ">131, dA[0.119,0.178] dB[0.106,0.098] g[12.295,13.389]\n",
            ">132, dA[0.251,0.193] dB[0.184,0.065] g[6.864,7.702]\n",
            ">133, dA[0.100,0.124] dB[0.072,0.094] g[6.089,7.256]\n",
            ">134, dA[0.121,0.118] dB[0.111,0.212] g[5.786,6.502]\n",
            ">135, dA[0.052,0.198] dB[0.200,0.141] g[7.113,7.418]\n",
            ">136, dA[0.169,0.079] dB[0.191,0.172] g[7.009,7.960]\n",
            ">137, dA[0.120,0.111] dB[0.081,0.437] g[5.577,6.110]\n",
            ">138, dA[0.089,0.057] dB[0.135,0.115] g[5.253,5.760]\n",
            ">139, dA[0.091,0.331] dB[0.071,0.036] g[7.570,6.763]\n",
            ">140, dA[0.094,0.154] dB[0.108,0.059] g[6.428,7.095]\n",
            ">141, dA[0.146,0.129] dB[0.083,0.116] g[7.132,8.265]\n",
            ">142, dA[0.120,0.127] dB[0.084,0.074] g[13.627,11.211]\n",
            ">143, dA[0.141,0.093] dB[0.147,0.463] g[6.782,7.260]\n",
            ">144, dA[0.155,0.141] dB[0.091,0.089] g[5.395,5.277]\n",
            ">145, dA[0.065,0.243] dB[0.067,0.049] g[7.735,7.457]\n",
            ">146, dA[0.076,0.065] dB[0.289,0.332] g[6.001,6.936]\n",
            ">147, dA[0.195,0.057] dB[0.228,0.187] g[5.431,5.931]\n",
            ">148, dA[0.099,0.361] dB[0.161,0.091] g[5.100,5.823]\n",
            ">149, dA[0.150,0.109] dB[0.078,0.103] g[6.016,7.866]\n",
            ">150, dA[0.157,0.068] dB[0.158,0.142] g[8.626,9.267]\n",
            ">151, dA[0.126,0.080] dB[0.076,0.154] g[14.243,12.309]\n",
            ">152, dA[0.096,0.075] dB[0.122,0.084] g[7.195,7.978]\n",
            ">153, dA[0.086,0.064] dB[0.238,0.156] g[8.101,8.243]\n",
            ">154, dA[0.115,0.138] dB[0.127,0.112] g[7.694,8.162]\n",
            ">155, dA[0.182,0.076] dB[0.098,0.114] g[11.615,10.653]\n",
            ">156, dA[0.248,0.092] dB[0.191,0.110] g[9.694,12.178]\n",
            ">157, dA[0.117,0.103] dB[0.048,0.069] g[13.920,12.090]\n",
            ">158, dA[0.131,0.090] dB[0.143,0.060] g[7.182,7.223]\n",
            ">159, dA[0.159,0.120] dB[0.060,0.079] g[6.373,6.584]\n",
            ">160, dA[0.072,0.072] dB[0.118,0.065] g[4.907,5.590]\n",
            ">161, dA[0.091,0.129] dB[0.066,0.155] g[8.660,11.253]\n",
            ">162, dA[0.264,0.192] dB[0.109,0.082] g[7.001,6.687]\n",
            ">163, dA[0.141,0.028] dB[0.099,0.069] g[6.345,6.786]\n",
            ">164, dA[0.091,0.121] dB[0.127,0.176] g[7.351,8.083]\n",
            ">165, dA[0.338,0.097] dB[0.140,0.226] g[8.416,7.960]\n",
            ">166, dA[0.084,0.187] dB[0.133,0.108] g[5.830,6.129]\n",
            ">167, dA[0.099,0.056] dB[0.316,0.175] g[7.537,7.389]\n",
            ">168, dA[0.121,0.224] dB[0.096,0.159] g[9.489,9.152]\n",
            ">169, dA[0.170,0.019] dB[0.071,0.113] g[8.585,8.659]\n",
            ">170, dA[0.125,0.747] dB[0.285,0.232] g[5.200,6.068]\n",
            ">171, dA[0.328,0.208] dB[0.119,0.330] g[7.714,8.389]\n",
            ">172, dA[0.126,0.329] dB[0.431,0.122] g[8.419,8.108]\n",
            ">173, dA[0.181,0.113] dB[0.146,0.172] g[6.889,6.876]\n",
            ">174, dA[0.357,0.144] dB[0.145,0.087] g[7.666,7.000]\n",
            ">175, dA[0.142,0.115] dB[0.160,0.160] g[5.677,6.795]\n",
            ">176, dA[0.137,0.324] dB[0.083,0.063] g[8.356,7.954]\n",
            ">177, dA[0.122,0.096] dB[0.108,0.074] g[6.025,6.868]\n",
            ">178, dA[0.115,0.146] dB[0.043,0.048] g[10.832,12.266]\n",
            ">179, dA[0.133,0.132] dB[0.106,0.064] g[6.193,7.092]\n",
            ">180, dA[0.087,0.111] dB[0.039,0.177] g[7.756,7.810]\n",
            ">181, dA[0.092,0.084] dB[0.038,0.111] g[14.545,12.743]\n",
            ">182, dA[0.194,0.124] dB[0.369,0.134] g[6.969,7.873]\n",
            ">183, dA[0.246,0.192] dB[0.086,0.127] g[6.492,6.537]\n",
            ">184, dA[0.139,0.103] dB[0.062,0.167] g[7.376,7.872]\n",
            ">185, dA[0.225,0.106] dB[0.258,0.026] g[6.906,7.067]\n",
            ">186, dA[0.113,0.116] dB[0.107,0.165] g[4.676,5.239]\n",
            ">187, dA[0.108,0.173] dB[0.140,0.062] g[4.989,6.214]\n",
            ">188, dA[0.128,0.095] dB[0.049,0.240] g[6.333,7.030]\n",
            ">189, dA[0.181,0.072] dB[0.091,0.055] g[6.589,7.583]\n",
            ">190, dA[0.274,0.209] dB[0.118,0.104] g[5.271,6.753]\n",
            ">191, dA[0.100,0.120] dB[0.082,0.131] g[6.583,8.229]\n",
            ">192, dA[0.151,0.091] dB[0.088,0.134] g[7.195,7.789]\n",
            ">193, dA[0.093,0.111] dB[0.057,0.111] g[10.287,10.406]\n",
            ">194, dA[0.143,0.110] dB[0.144,0.136] g[11.330,10.083]\n",
            ">195, dA[0.042,0.119] dB[0.110,0.162] g[6.217,7.006]\n",
            ">196, dA[0.095,0.030] dB[0.174,0.158] g[7.429,8.215]\n",
            ">197, dA[0.090,0.060] dB[0.113,0.070] g[6.347,7.554]\n",
            ">198, dA[0.176,0.072] dB[0.145,0.081] g[7.684,7.326]\n",
            ">199, dA[0.037,0.052] dB[0.322,0.126] g[6.158,6.982]\n",
            ">200, dA[0.048,0.061] dB[0.145,0.116] g[9.269,9.397]\n",
            ">201, dA[0.039,0.101] dB[0.273,0.067] g[6.112,5.866]\n",
            ">202, dA[0.055,0.052] dB[0.099,0.113] g[11.158,9.733]\n",
            ">203, dA[0.042,0.038] dB[0.143,0.180] g[7.435,8.193]\n",
            ">204, dA[0.092,0.058] dB[0.302,0.180] g[5.581,6.515]\n",
            ">205, dA[0.107,0.147] dB[0.080,0.129] g[6.702,6.863]\n",
            ">206, dA[0.085,0.036] dB[0.121,0.044] g[6.527,7.172]\n",
            ">207, dA[0.128,0.082] dB[0.057,0.078] g[5.914,6.490]\n",
            ">208, dA[0.065,0.021] dB[0.026,0.085] g[11.119,10.019]\n",
            ">209, dA[0.032,0.055] dB[0.181,0.130] g[6.160,7.347]\n",
            ">210, dA[0.183,0.198] dB[0.055,0.062] g[7.259,7.181]\n",
            ">211, dA[0.086,0.034] dB[0.089,0.160] g[6.859,7.794]\n",
            ">212, dA[0.174,0.077] dB[0.233,0.173] g[5.917,6.915]\n",
            ">213, dA[0.045,0.053] dB[0.088,0.124] g[6.278,7.287]\n",
            ">214, dA[0.029,0.051] dB[0.064,0.120] g[7.505,7.971]\n",
            ">215, dA[0.071,0.033] dB[0.049,0.094] g[6.749,7.542]\n",
            ">216, dA[0.034,0.058] dB[0.157,0.039] g[6.451,7.322]\n",
            ">217, dA[0.097,0.109] dB[0.057,0.078] g[6.314,7.759]\n",
            ">218, dA[0.243,0.220] dB[0.036,0.278] g[6.408,7.107]\n",
            ">219, dA[0.039,0.069] dB[0.183,0.048] g[6.931,7.554]\n",
            ">220, dA[0.073,0.061] dB[0.057,0.033] g[6.162,6.729]\n",
            ">221, dA[0.045,0.017] dB[0.047,0.105] g[7.274,7.550]\n",
            ">222, dA[0.177,0.157] dB[0.102,0.078] g[5.251,5.564]\n",
            ">223, dA[0.056,0.093] dB[0.060,0.193] g[5.821,6.651]\n",
            ">224, dA[0.062,0.103] dB[0.046,0.086] g[5.920,6.449]\n",
            ">225, dA[0.134,0.047] dB[0.142,0.059] g[8.468,8.369]\n",
            ">226, dA[0.047,0.100] dB[0.115,0.066] g[6.763,7.766]\n",
            ">227, dA[0.030,0.102] dB[0.048,0.127] g[6.318,6.370]\n",
            ">228, dA[0.159,0.068] dB[0.106,0.161] g[5.278,6.123]\n",
            ">229, dA[0.127,0.026] dB[0.235,0.067] g[8.372,6.993]\n",
            ">230, dA[0.095,0.074] dB[0.090,0.154] g[5.634,5.747]\n",
            ">231, dA[0.094,0.099] dB[0.065,0.100] g[14.334,12.481]\n",
            ">232, dA[0.124,0.123] dB[0.224,0.042] g[11.292,12.363]\n",
            ">233, dA[0.045,0.022] dB[0.115,0.194] g[6.348,7.484]\n",
            ">234, dA[0.041,0.018] dB[0.082,0.268] g[7.267,8.880]\n",
            ">235, dA[0.042,0.033] dB[0.420,0.229] g[6.717,6.875]\n",
            ">236, dA[0.068,0.210] dB[0.089,0.119] g[5.630,6.374]\n",
            ">237, dA[0.118,0.048] dB[0.192,0.090] g[5.854,6.488]\n",
            ">238, dA[0.154,0.188] dB[0.049,0.146] g[4.664,5.079]\n",
            ">239, dA[0.036,0.172] dB[0.313,0.133] g[7.269,6.499]\n",
            ">240, dA[0.126,0.026] dB[0.096,0.125] g[6.869,8.474]\n",
            ">241, dA[0.046,0.023] dB[0.045,0.096] g[6.120,7.653]\n",
            ">242, dA[0.127,0.106] dB[0.179,0.033] g[6.112,5.895]\n",
            ">243, dA[0.034,0.167] dB[0.053,0.130] g[5.423,5.249]\n",
            ">244, dA[0.025,0.088] dB[0.144,0.079] g[8.760,8.687]\n",
            ">245, dA[0.095,0.041] dB[0.187,0.120] g[6.714,7.949]\n",
            ">246, dA[0.181,0.229] dB[0.112,0.142] g[5.392,5.962]\n",
            ">247, dA[0.053,0.073] dB[0.080,0.050] g[8.616,8.940]\n",
            ">248, dA[0.122,0.030] dB[0.150,0.236] g[6.126,8.033]\n",
            ">249, dA[0.100,0.055] dB[0.050,0.109] g[7.253,7.135]\n",
            ">250, dA[0.053,0.046] dB[0.041,0.040] g[7.075,6.347]\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            ">Saved: g_model_AtoB_000250.h5 and g_model_BtoA_000250.h5\n"
          ]
        }
      ],
      "source": [
        "# load image data\n",
        "dataset = load_real_samples('underwater.npz')\n",
        "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
        "# define input shape based on the loaded dataset\n",
        "image_shape = dataset[0].shape[1:]\n",
        "# generator: A -> \n",
        "g_model_AtoB = define_generator(image_shape)\n",
        "# generator: B -> A\n",
        "g_model_BtoA = define_generator(image_shape)\n",
        "# discriminator: A -> [real/fake]\n",
        "d_model_A = define_discriminator(image_shape)\n",
        "# discriminator: B -> [real/fake]\n",
        "d_model_B = define_discriminator(image_shape)\n",
        "# composite: A -> B -> [real/fake, A]\n",
        "c_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)\n",
        "# composite: B -> A -> [real/fake, B]\n",
        "c_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)\n",
        "# train models\n",
        "train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u23EVSUfRvt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow import keras\n",
        "# from tensorflow.keras import layers, Dense, Input, InputLayer, Flatten\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from  matplotlib import pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# img_folder = 'C:/Users/dell/Downloads/underwater_imagenet/trainA'\n",
        "img_folder=path\n",
        "plt.figure(figsize=(20,20))\n",
        "# test_folder=r'CV\\Intel_Images\\seg_train\\seg_train\\forest'\n",
        "for i in range(5):\n",
        "    file = random.choice(os.listdir(img_folder))\n",
        "    image_path= os.path.join(img_folder, file)\n",
        "    img=mpimg.imread(image_path)\n",
        "    ax=plt.subplot(1,5,i+1)\n",
        "    ax.title.set_text(file)\n",
        "    plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfHdfQwPPU-m"
      },
      "outputs": [],
      "source": [
        " \n",
        "data_list = list()\n",
        "size=(256,256)\n",
        "# enumerate filenames in directory, assume all are images\n",
        "for filename in listdir(path):\n",
        "# load and resize the image\n",
        "     pixels = load_img(path + filename, target_size=size)\n",
        "# convert to numpy array\n",
        "     pixels = img_to_array(pixels)\n",
        "# store\n",
        "     data_list.append(pixels)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ROuNhivZtGWs"
      },
      "outputs": [],
      "source": [
        "# example of using saved cyclegan models for image translation\n",
        "from keras.models import load_model\n",
        "from numpy import load\n",
        "from numpy import vstack\n",
        "from matplotlib import pyplot\n",
        "from numpy.random import randint\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "s9x3npNZS3Wc"
      },
      "outputs": [],
      "source": [
        "# load and prepare training images\n",
        "def load_real_samples(filename):\n",
        "# load the dataset\n",
        "  data = load(filename)\n",
        "# unpack arrays\n",
        "  X1, X2 = data['arr_0'], data['arr_1']\n",
        "# scale from [0,255] to [-1,1]\n",
        "  X1 = (X1 - 127.5) / 127.5\n",
        "  X2 = (X2 - 127.5) / 127.5\n",
        "  return [X1, X2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select a random sample of images from the dataset\n",
        "def select_sample(dataset, n_samples):\n",
        "# choose random instances\n",
        "   ix = randint(0, dataset.shape[0], n_samples)\n",
        "# retrieve selected images\n",
        "   X = dataset[ix]\n",
        "   return "
      ],
      "metadata": {
        "id": "7YO2K_qCsoyn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the image, the translation, and the reconstruction\n",
        "def show_plot(imagesX, imagesY1, imagesY2):\n",
        "   images = vstack((imagesX, imagesY1, imagesY2))\n",
        "   titles = ['Real', 'Generated', 'Reconstructed']\n",
        "# scale from [-1,1] to [0,1]\n",
        "   images = (images + 1) / 2.0\n",
        "# plot images row by row\n",
        "   for i in range(len(images)):\n",
        "# define subplot\n",
        "      pyplot.subplot(1, len(images), 1 + i)\n",
        "# turn off axis\n",
        "      pyplot.axis('off')\n",
        "# plot raw pixel data\n",
        "      pyplot.imshow(images[i])\n",
        "# title\n",
        "      pyplot.title(titles[i])\n",
        "   pyplot.show()\n"
      ],
      "metadata": {
        "id": "TC1Ij5-csrzg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "A_data, B_data = load_real_samples('underwater.npz')\n",
        "print('Loaded', A_data.shape, B_data.shape)\n",
        "# load the models\n",
        "cust = {'InstanceNormalization': InstanceNormalization}\n",
        "model_AtoB = load_model('g_model_AtoB_000250.h5', cust)\n",
        "model_BtoA = load_model('g_model_BtoA_000250.h5', cust)\n",
        "# plot A->B->A\n",
        "A_real = select_sample(A_data, 1)\n",
        "# B_generated = model_AtoB.predict(A_real)\n",
        "# A_reconstructed = model_BtoA.predict(B_generated)\n",
        "B_generated = g_model_AtoB.predict(A_real)\n",
        "A_reconstructed = g_model_BtoA.predict(B_generated)\n",
        "show_plot(A_real, B_generated, A_reconstructed)\n",
        "# plot B->A->B\n",
        "B_real = select_sample(B_data, 1)\n",
        "A_generated = model_BtoA.predict(B_real)\n",
        "B_reconstructed = model_AtoB.predict(A_generated)\n",
        "show_plot(B_real, A_generated, B_reconstructed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "AjpSWodfs-ay",
        "outputId": "290eea15-6e54-4b9e-bb18-354520a07825"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded (25, 256, 256, 3) (25, 256, 256, 3)\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-6069d0e2a0c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# B_generated = model_AtoB.predict(A_real)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# A_reconstructed = model_BtoA.predict(B_generated)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mB_generated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_model_AtoB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mA_reconstructed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_model_BtoA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_generated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mshow_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_generated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_reconstructed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 991\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    992\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m     raise RuntimeError(\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'NoneType'>, <class 'NoneType'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sCE0l-6EueRT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}